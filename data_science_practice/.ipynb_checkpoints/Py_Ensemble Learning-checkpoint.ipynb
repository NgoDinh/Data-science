{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning:\n",
    "x --> (averaging method|boosting method) --> better result\n",
    "- averaging method: Votting, bagging-pasting, Random forest( baging method of decision tree)\n",
    "- boosting method:\n",
    "___\n",
    "- Note:\n",
    ">- Ensemble methods work best when the predictors are as independent from one another as possible(Random Subspaces)\n",
    "___\n",
    "- Resources:\n",
    ">- [sklearn_document](http://scikit-learn.org/stable/modules/ensemble.html#ensemble)\n",
    ">- Book: Hands_On_Machine_Learning_with_Scikit_Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## VotingClassifier\n",
    "> Besides hard voting we can use [soft_voting](http://scikit-learn.org/stable/modules/ensemble.html#ensemble) and voting with [GridSerch](http://scikit-learn.org/stable/modules/ensemble.html#ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "#---------------------------------------------------#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "# std = StandardScaler()\n",
    "# X = std.fit_transform(X)\n",
    "\n",
    "# plf = PolynomialFeatures(degree=3)\n",
    "# X = plf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing model\n",
    "#---------------------------------------------------#\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "log_clf = LogisticRegression()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svm_clf), ('rf', rnd_clf)],\n",
    "    voting='hard',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier (0.92666666666666653, 0.057348835113617498)\n",
      "SVC (0.95333333333333337, 0.039999999999999994)\n",
      "LogisticRegression (0.90000000000000002, 0.047140452079103161)\n",
      "VotingClassifier (0.95333333333333337, 0.039999999999999994)\n"
     ]
    }
   ],
   "source": [
    "#Singel model vs Voting\n",
    "#---------------------------------------------------#\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for cfm in (rnd_clf, svm_clf, log_clf, voting_clf):\n",
    "    scores = cross_val_score(cfm, X, y, cv=5, scoring='accuracy')\n",
    "    print(cfm.__class__.__name__,\n",
    "         (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    ">-  Sampling both training instances and features is called the Random Patches method\n",
    ">-  Keeping all training instances but sampling features is called the Random Subspaces method( to reduce the correlation between estimators in an ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "0.913333333333\n"
     ]
    }
   ],
   "source": [
    "#Singel model vs Bagging\n",
    "#---------------------------------------------------#\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    max_samples=0.9,\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# change bootstrap = False to have pasting model\n",
    "print(cross_val_score(bag_clf, X, y, cv=5, scoring='accuracy').mean())\n",
    "print(cross_val_score(DecisionTreeClassifier(), X, y, cv=5, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93999999999999995"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out-of-bag(oob) instances, we can use it to test model\n",
    "#---------------------------------------------------#\n",
    "\n",
    "bag_clf_oob = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\"), # spliter to identify how to choose features ( best or random)\n",
    "    max_samples=0.9,\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")\n",
    "bag_clf_oob.fit(X, y)\n",
    "bag_clf_oob.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "#---------------------------------------------------#\n",
    "\n",
    "rdf = RandomForestClassifier(n_estimators=500,\n",
    "                            max_leaf_nodes=16,\n",
    "                            n_jobs=-1\n",
    "                            )\n",
    "\n",
    "#Random forest = bagging + decision tree\n",
    "\n",
    "print(cross_val_score(rdf, X, y, cv=5, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946666666667\n"
     ]
    }
   ],
   "source": [
    "# in decision tree we will find best threshold to split\n",
    "# to reduce variance error besides random features (spliter=random), we can random threshold too\n",
    "# this model is called extra ExtraTreesRegressor \n",
    "#---------------------------------------------------#\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(n_estimators=10,\n",
    "                              max_depth=None,\n",
    "                              min_samples_split=2,\n",
    "                              random_state=0)\n",
    "\n",
    "print(cross_val_score(rdf, X, y, cv=5, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.103592420779\n",
      "sepal width (cm) 0.0257376648273\n",
      "petal length (cm) 0.432858559295\n",
      "petal width (cm) 0.437811355099\n"
     ]
    }
   ],
   "source": [
    "#Feature importance with random forest\n",
    "#---------------------------------------------------#\n",
    "\n",
    "rdf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rdf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X --> data set(t) --> model(t) --> error rate --> alpha of model(t) --> update d of instance -->come back(step 2) ==> final result base on w of model\n",
    "> - How to take a data set: replacement ( same size as original data + you can select the\n",
    "same  example  more  than  once)\n",
    "> - Error rate(e) : is just the number of misclassifications over the training set divided by the training set size.\n",
    "> - Alpha of model: $$ \\alpha_t = \\frac{1}{2}*ln(\\frac{1 - e}{e}) $$\n",
    "> - Update d: $$ D_{t+1}(i) = D_t(i)*exp(\\frac{-\\alpha_t*y_i*h(x_i)}{z_t}) $$\n",
    "> - Final result: $$ H_f = sign(\\sum(\\alpha_t*h_t(x)) $$\n",
    "\n",
    "Error rate vs Alpha : Reciprocal ratio\n",
    "___\n",
    "- Note:\n",
    "> - Why update d of a instance can make model predict exactly label of this instance.\n",
    "> - SAME vs SAME.R: SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. ???\n",
    "___\n",
    "- Resource:\n",
    "> - [mccormickml](http://mccormickml.com/2013/12/13/adaboost-tutorial/)\n",
    "> - [analyticsvidhya](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)\n",
    "> - [sklearn_example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py)\n",
    "> - Book: Machine learning in action & hands on machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmr/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/home/mmr/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e342236a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error rate vs alpha\n",
    "#---------------------------------------------------#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "E_trial = np.linspace(0, 1, 100)\n",
    "alpha_trial = np.log((1 - E_trial) / E_trial) / 2\n",
    "\n",
    "plt.plot(E_trial, alpha_trial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0e342372e8>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e258330b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#alpha vs D\n",
    "#---------------------------------------------------#\n",
    "\n",
    "e_power_alpha = np.exp(alpha_trial) #if incorrectly \n",
    "e_power_neg_alpha = np.exp(-alpha_trial) #if correctly predicted\n",
    "\n",
    "plt.plot(alpha_trial, e_power_alpha, 'g-')\n",
    "plt.plot(alpha_trial, e_power_neg_alpha, 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#Adaboost\n",
    "#---------------------------------------------------#\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
    "                             n_estimators=600,\n",
    "                            learning_rate=1.5,\n",
    "                            algorithm='SAMME.R')\n",
    "\n",
    "print(cross_val_score(ada_clf, X, y, cv=5, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this algorithm is finding a h(x) function to boosting F(x) function\n",
    "> $$ F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n L(y_i, \\gamma) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n (\\gamma - y_i)^2 = {\\displaystyle {\\frac {1}{n}}\\sum _{i=1}^{n}y_{i}.} $$\n",
    "> $$ F_{m+1}(x) = F_m(x) + h_m(x) = y $$\n",
    "> $$ F(x) = F_1(x) \\mapsto F_2(x) = F_1(x) + h_1(x) \\dots \\mapsto F_M(x) = F_{M-1}(x) + h_{M-1}(x) $$\n",
    "\n",
    "___\n",
    "\n",
    "- Note:\n",
    "> - How to select the best value for the model’s hyper-parameter m: testing different values of m via cross-validation\n",
    "> - [How to use XGboost package to perform tree_base model](https://github.com/dataworkshop/xgboost)\n",
    "___\n",
    "- Resources:\n",
    "> - [Kaggle](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "> - Hands on machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75026781]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy.random as rnd\n",
    "\n",
    "# training set: a noisy quadratic function\n",
    "#---------------------------------------------------#\n",
    "rnd.seed(42)\n",
    "X = rnd.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * rnd.randn(100)\n",
    "\n",
    "# train Regressor\n",
    "#---------------------------------------------------#\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "# now train 2nd Regressor using errors made by 1st one.\n",
    "#---------------------------------------------------#\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "# now train 3rd Regressor using errors made by 2nd one.\n",
    "#---------------------------------------------------#\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "X_new = np.array([[0.8]])\n",
    "\n",
    "# now have ensemble w/ three trees.\n",
    "#---------------------------------------------------#\n",
    "y_pred = sum(tree.predict(X_new) for tree in (\n",
    "    tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient boosting in sklearn.\n",
    "#---------------------------------------------------#\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(\n",
    "    max_depth=2, \n",
    "    n_estimators=3, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42)\n",
    "\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find number of trees\n",
    "#---------------------------------------------------#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gbrt_1 = GradientBoostingRegressor(max_depth=2,\n",
    "                                   n_estimators=120)\n",
    "gbrt_1.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_test, y_pred)\n",
    "         for y_pred in  gbrt_1.staged_predict(X_test)]\n",
    "best = np.argmin(errors)\n",
    "best\n",
    "#besides we can use early stopping to find number of tree to train model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X \\underset{f_1(X),f_2(X),f_3(X)}{\\mapsto} y_1,y_2,y_3 \\underset{f_1(Y),f_2(Y),f_3(Y)}{\\mapsto} y $$\n",
    "- Note:\n",
    "> - 3 predictor and 2 layer is just example, in real we can select another number\n",
    "\n",
    "- Resource:\n",
    "> - Hands on machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
