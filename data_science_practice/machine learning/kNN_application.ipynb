{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is kNN\n",
    "Main idea behind KNN is given a query point we will see the k neighbours and after that our object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors.\n",
    "\n",
    "### KNN modeling works around four parameters:\n",
    "\n",
    "- Features: The variables based on which similarity between two points is calculated\n",
    "- Distance function: Distance metric to be used for computing similarity between points\n",
    "- Neighborhood (k) : Number of neighbors to search for\n",
    "- Scoring function: The function which combines the labels of the neighbors to form a single score for the query point\n",
    "\n",
    "### Preparing data for kNN\n",
    "\n",
    "- Rescale Data: KNN performs much better if all of the data has the same scale. Normalizing your data to the range is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.\n",
    "- Address Missing Data: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.\n",
    "- Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.\n",
    "\n",
    "### <mark>Distance method ??????? k??????</mark>\n",
    "\n",
    "- Manhattan Distance used for continuous variables.\n",
    "- Hamming Distance and Euclidean Distance used for categorical variables\n",
    "\n",
    "### Related knowledge\n",
    "\n",
    "- Instance-Based Learning: Storing and using specific instances improves the performance of several supervised learning algorithms.(k-nearest neighbor algorithm, kernel machines and RBF networks)\n",
    "- lazy learning: is a learning method in which generalization beyond the training data is delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries.\n",
    "- Nonparametric\n",
    "\n",
    "\n",
    "# Strongth and weakness of kNN\n",
    "- advantage: Robust to <mark>noisy data</mark>\n",
    "- disadvantage: Need to determine k, distance method. Computation cost is quite high.\n",
    "\n",
    "\n",
    "# Type of problem can use kNN to solve\n",
    "- Classification: When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances.\n",
    "- Regression: When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.\n",
    "\n",
    "# Learn it later\n",
    "- distance measures : Euclidean,Manhathan,Hamming, Jaccard, Levenshtein.\n",
    "- Probabilistic data-structure LSH.LSH(Locality sensitive hashing) hashes\n",
    "- QuadTrees\n",
    "- Exhaustive Search, Exhaustive large search and Voronoi Partitioning.\n",
    "- <mark>Artificial neural networks - Decision trees - Ensemble learning</mark> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
